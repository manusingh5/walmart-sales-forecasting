{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b037bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd27314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a5d1b",
   "metadata": {},
   "source": [
    "# Command used for putting raw data in HDFS\n",
    "## hdfs dfs -put shared/Walmart-Retail-Dataset.csv /user/talentum/DBDA_Project/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937e7d6",
   "metadata": {},
   "source": [
    "# Loading raw data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa6c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').options(Header=True, inferSchema=True).load('DBDA_Project/Walmart-Retail-Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b9456c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041860"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6614c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city', 'customer_age', 'customer_name', 'customer_segment', 'discount', 'order_date', 'order_id', 'order_priority', 'order_quantity', 'product_base_margin', 'product_category', 'product_container', 'product_name', 'product_sub_category', 'profit', 'region', 'sales', 'ship_date', 'ship_mode', 'shipping_cost', 'state', 'unit_price', 'zip_code', 'adjusted_col', '_c24', '_c25', '_c26', '_c27', '_c28']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb704121",
   "metadata": {},
   "source": [
    "# Dropping irrelevant columns -> df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb24492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.drop('_c24', '_c25', '_c26', '_c27', '_c28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edd98e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city', 'customer_age', 'customer_name', 'customer_segment', 'discount', 'order_date', 'order_id', 'order_priority', 'order_quantity', 'product_base_margin', 'product_category', 'product_container', 'product_name', 'product_sub_category', 'profit', 'region', 'sales', 'ship_date', 'ship_mode', 'shipping_cost', 'state', 'unit_price', 'zip_code', 'adjusted_col']\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c48eb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041860"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a99370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|product_sub_category|\n",
      "+--------------------+\n",
      "|           Envelopes|\n",
      "|        Rubber Bands|\n",
      "|      Delivery Truck|\n",
      "|  Chairs & Chairmats|\n",
      "|                  \\N|\n",
      "|                null|\n",
      "|     Copiers and Fax|\n",
      "|           Bookcases|\n",
      "| Pens & Art Supplies|\n",
      "|              Labels|\n",
      "|         71.97487538|\n",
      "|Computer Peripherals|\n",
      "|               Paper|\n",
      "|          Appliances|\n",
      "|         Express Air|\n",
      "|              Tables|\n",
      "|            Scissors|\n",
      "|  Storage & Organiz\"|\n",
      "|  Office Furnishings|\n",
      "|     Office Machines|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df1.select(\"product_sub_category\").distinct().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72522d70",
   "metadata": {},
   "source": [
    "# Deleting '\\N' values -> df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a07ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Define the value to be deleted\n",
    "value_to_delete = \"\\\\N\"\n",
    "\n",
    "df2 = df1.filter(\n",
    "    ~(\n",
    "        (col(\"city\") == value_to_delete) |\n",
    "        (col(\"customer_age\") == value_to_delete) |\n",
    "        (col(\"customer_name\") == value_to_delete)|\n",
    "        (col(\"customer_segment\") == value_to_delete)|\n",
    "        (col(\"discount\") == value_to_delete)|\n",
    "        (col(\"order_date\") == value_to_delete)|\n",
    "        (col(\"order_id\") == value_to_delete)|\n",
    "        (col(\"order_priority\") == value_to_delete)|\n",
    "        (col(\"order_quantity\") == value_to_delete)|\n",
    "        (col(\"product_base_margin\") == value_to_delete)|\n",
    "        (col(\"product_category\") == value_to_delete)|\n",
    "        (col(\"product_container\") == value_to_delete)|\n",
    "        (col(\"product_name\") == value_to_delete)|\n",
    "        (col(\"product_sub_category\") == value_to_delete)|\n",
    "        (col(\"profit\") == value_to_delete)|\n",
    "        (col(\"region\") == value_to_delete)|\n",
    "        (col(\"sales\") == value_to_delete)|\n",
    "        (col(\"ship_date\") == value_to_delete)|\n",
    "        (col(\"shipping_cost\") == value_to_delete)|\n",
    "        (col(\"state\") == value_to_delete)|\n",
    "        (col(\"unit_price\") == value_to_delete)|\n",
    "        (col(\"zip_code\") == value_to_delete))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce89f65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020437"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b80030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = df2.select(\"product_sub_category\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d4266ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|product_sub_category|\n",
      "+--------------------+\n",
      "|           Envelopes|\n",
      "|        Rubber Bands|\n",
      "|  Chairs & Chairmats|\n",
      "|     Copiers and Fax|\n",
      "|           Bookcases|\n",
      "| Pens & Art Supplies|\n",
      "|              Labels|\n",
      "|Computer Peripherals|\n",
      "|               Paper|\n",
      "|          Appliances|\n",
      "|              Tables|\n",
      "|            Scissors|\n",
      "|  Office Furnishings|\n",
      "|     Office Machines|\n",
      "|Storage & Organiz...|\n",
      "|Telephones and Co...|\n",
      "|Binders and Binde...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_names.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00d66c",
   "metadata": {},
   "source": [
    "# Shifting Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3ca52",
   "metadata": {},
   "source": [
    "# 1. Concating columns to get Scissors and Rulers together ->df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c77805e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, concat, col, lit\n",
    "df3 = df2.withColumn(\n",
    "    \"product_sub_category\",\n",
    "    when(col(\"product_sub_category\") == \"Scissors\", concat(col(\"product_sub_category\"), lit(\",\"), col(\"profit\"))).otherwise(col(\"product_sub_category\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec315d9c",
   "metadata": {},
   "source": [
    "# 2. Shifting Columns ->df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b654e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, concat, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dacc5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition2 = col(\"product_sub_category\") == \"Scissors, Rulers and Trimmers\"\n",
    "\n",
    "\n",
    "df4 = df3.withColumn(\"profit\",\n",
    "    when(condition2,  col(\"region\")).otherwise(col(\"profit\"))).withColumn(\"region\",\n",
    "    when(condition2,  col(\"sales\")).otherwise(col(\"region\"))).withColumn(\"sales\",\n",
    "    when(condition2,  col(\"ship_date\")).otherwise(col(\"sales\"))).withColumn(\"ship_date\",\n",
    "    when(condition2,  col(\"ship_mode\")).otherwise(col(\"ship_date\"))).withColumn(\"ship_mode\",\n",
    "    when(condition2,  col(\"shipping_cost\")).otherwise(col(\"ship_mode\"))).withColumn(\"shipping_cost\",\n",
    "    when(condition2,  col(\"state\")).otherwise(col(\"shipping_cost\"))).withColumn(\"state\",\n",
    "    when(condition2,  col(\"unit_price\")).otherwise(col(\"state\"))).withColumn(\"unit_price\",\n",
    "    when(condition2,  col(\"zip_code\")).otherwise(col(\"unit_price\"))).withColumn(\"zip_code\",\n",
    "    when(condition2,  col(\"adjusted_col\")).otherwise(col(\"zip_code\")))\n",
    "      \n",
    "                                       \n",
    "                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df574aae",
   "metadata": {},
   "source": [
    "# Changing Newport values df->5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366d6c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(region='South'), Row(region='Central'), Row(region='East'), Row(region='40.2'), Row(region='West')]\n"
     ]
    }
   ],
   "source": [
    "print(df4.select(\"region\").distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60688f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(state='Utah'), Row(state='Minnesota'), Row(state='Ohio'), Row(state='Oregon'), Row(state='Arkansas'), Row(state='Texas'), Row(state='North Dakota'), Row(state='Pennsylvania'), Row(state='Connecticut'), Row(state='Nebraska'), Row(state='Vermont'), Row(state='Nevada'), Row(state='Washington'), Row(state='Illinois'), Row(state='Oklahoma'), Row(state='MO'), Row(state='New Mexico'), Row(state='West Virginia'), Row(state='Rhode Island'), Row(state='Georgia'), Row(state='Montana'), Row(state='Michigan'), Row(state='Virginia'), Row(state='North Carolina'), Row(state='Wyoming'), Row(state='Kansas'), Row(state='New Jersey'), Row(state='Maryland'), Row(state='Alabama'), Row(state='Arizona'), Row(state='Iowa'), Row(state='Kentucky'), Row(state='Louisiana'), Row(state='1.22'), Row(state='Mississippi'), Row(state='Tennessee'), Row(state='New Hampshire'), Row(state='MA'), Row(state='Florida'), Row(state='Indiana'), Row(state='Idaho'), Row(state='South Carolina'), Row(state='South Dakota'), Row(state='California'), Row(state='New York'), Row(state='Wisconsin'), Row(state='Colorado'), Row(state='Maine')]\n"
     ]
    }
   ],
   "source": [
    "print(df4.select(\"state\").distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "019987e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.withColumn(\"region\", when(col(\"city\") == \"Newport\", \"South\").otherwise(col(\"region\"))).withColumn(\"state\", when(col(\"city\") == \"Newport\", \"Rhode Island\").otherwise(col(\"state\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f975c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(region='South'), Row(region='Central'), Row(region='East'), Row(region='West')]\n"
     ]
    }
   ],
   "source": [
    "print(df5.select(\"region\").distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cb621ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(region='South'), Row(region='Central'), Row(region='East'), Row(region='40.2'), Row(region='West')]\n"
     ]
    }
   ],
   "source": [
    "print(df4.select(\"region\").distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bfdb395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(state='Utah'), Row(state='Minnesota'), Row(state='Ohio'), Row(state='Oregon'), Row(state='Arkansas'), Row(state='Texas'), Row(state='North Dakota'), Row(state='Pennsylvania'), Row(state='Connecticut'), Row(state='Nebraska'), Row(state='Vermont'), Row(state='Nevada'), Row(state='Washington'), Row(state='Illinois'), Row(state='Oklahoma'), Row(state='MO'), Row(state='New Mexico'), Row(state='West Virginia'), Row(state='Rhode Island'), Row(state='Georgia'), Row(state='Montana'), Row(state='Michigan'), Row(state='Virginia'), Row(state='North Carolina'), Row(state='Wyoming'), Row(state='Kansas'), Row(state='New Jersey'), Row(state='Maryland'), Row(state='Alabama'), Row(state='Arizona'), Row(state='Iowa'), Row(state='Kentucky'), Row(state='Louisiana'), Row(state='Mississippi'), Row(state='Tennessee'), Row(state='New Hampshire'), Row(state='MA'), Row(state='Florida'), Row(state='Indiana'), Row(state='Idaho'), Row(state='South Carolina'), Row(state='South Dakota'), Row(state='California'), Row(state='New York'), Row(state='Wisconsin'), Row(state='Colorado'), Row(state='Maine')]\n"
     ]
    }
   ],
   "source": [
    "print(df5.select(\"state\").distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "460138ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(df4.select(\"state\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6745669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "print(df5.select(\"state\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f7de9",
   "metadata": {},
   "source": [
    "# Null Values treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf0fc6",
   "metadata": {},
   "source": [
    "# 1. Finding Null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a5efc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'city' has 0 null values\n",
      "Column 'customer_age' has 0 null values\n",
      "Column 'customer_name' has 0 null values\n",
      "Column 'customer_segment' has 0 null values\n",
      "Column 'discount' has 0 null values\n",
      "Column 'order_date' has 0 null values\n",
      "Column 'order_id' has 0 null values\n",
      "Column 'order_priority' has 0 null values\n",
      "Column 'order_quantity' has 0 null values\n",
      "Column 'product_base_margin' has 0 null values\n",
      "Column 'product_category' has 0 null values\n",
      "Column 'product_container' has 0 null values\n",
      "Column 'product_name' has 0 null values\n",
      "Column 'product_sub_category' has 0 null values\n",
      "Column 'profit' has 0 null values\n",
      "Column 'region' has 0 null values\n",
      "Column 'sales' has 0 null values\n",
      "Column 'ship_date' has 0 null values\n",
      "Column 'ship_mode' has 3378 null values\n",
      "Column 'shipping_cost' has 0 null values\n",
      "Column 'state' has 0 null values\n",
      "Column 'unit_price' has 0 null values\n",
      "Column 'zip_code' has 0 null values\n",
      "Column 'adjusted_col' has 1002853 null values\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Declaring list to hold the null counts for each column\n",
    "null_counts = []\n",
    "\n",
    "# Iterating over each column in the DataFrame\n",
    "for column in df5.columns:\n",
    "    null_count = df5.select(sum(col(column).isNull().cast(\"int\")).alias(column)).collect()[0][0]\n",
    "    null_counts.append((column, null_count))\n",
    "\n",
    "# Printing the null counts for each column\n",
    "for column, null_count in null_counts:\n",
    "    print(f\"Column '{column}' has {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22a00be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|     ship_mode| count|\n",
      "+--------------+------+\n",
      "|Delivery Truck|336845|\n",
      "|   Regular Air|339630|\n",
      "|          null|  3378|\n",
      "|   Express Air|340584|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping by the column with null values and counting the occurrences\n",
    "unique_value_counts = df5.groupBy(\"ship_mode\").count()\n",
    "\n",
    "# Showing the result\n",
    "unique_value_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439712f",
   "metadata": {},
   "source": [
    "# 1.First Dropping 'adjusted_col' Column->df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c09b7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.drop(\"adjusted_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "781ed97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city', 'customer_age', 'customer_name', 'customer_segment', 'discount', 'order_date', 'order_id', 'order_priority', 'order_quantity', 'product_base_margin', 'product_category', 'product_container', 'product_name', 'product_sub_category', 'profit', 'region', 'sales', 'ship_date', 'ship_mode', 'shipping_cost', 'state', 'unit_price', 'zip_code']\n"
     ]
    }
   ],
   "source": [
    "print(df6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71a67354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, city: string, customer_age: string, customer_name: string, customer_segment: string, discount: string, order_date: string, order_id: string, order_priority: string, order_quantity: string, product_base_margin: string, product_category: string, product_container: string, product_name: string, product_sub_category: string, profit: string, region: string, sales: string, ship_date: string, ship_mode: string, shipping_cost: string, state: string, unit_price: string, zip_code: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a6cc2",
   "metadata": {},
   "source": [
    "# 2.Dropping Null values->df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "504049b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020437"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1c393d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df6.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65d83337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1017059"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "967e93f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3378 0\n"
     ]
    }
   ],
   "source": [
    "null_count6 = df6.filter(col(\"ship_mode\").isNull()).count()\n",
    "null_count7 = df7.filter(col(\"ship_mode\").isNull()).count()\n",
    "\n",
    "print(null_count6,null_count7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca57288",
   "metadata": {},
   "source": [
    "# Changing datatype of order_date ->df8 and removing null values formed ->df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5dcac5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofmonth\n",
    "\n",
    "#Converting string to date type for order_date\n",
    "df8 = df7.withColumn(\"order_date\", to_date(df[\"order_date\"], \"dd-MM-yyyy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b0ed89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1938\n"
     ]
    }
   ],
   "source": [
    "null_count8 = df8.filter(col(\"order_date\").isNull()).count()\n",
    "\n",
    "print(null_count8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "701b3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9=df8.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4422105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "null_count9 = df9.filter(col(\"order_date\").isNull()).count()\n",
    "print(null_count9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6276413",
   "metadata": {},
   "source": [
    "# Removing rows having year 2023 from order_date for further use in model ->df10 -> df11 -> df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c3bd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = df9.withColumn('year', year(df9['order_date']))\n",
    "df11 = df10.filter(df10['year'] != 2023)\n",
    "df12 = df11.drop('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3be5503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city',\n",
       " 'customer_age',\n",
       " 'customer_name',\n",
       " 'customer_segment',\n",
       " 'discount',\n",
       " 'order_date',\n",
       " 'order_id',\n",
       " 'order_priority',\n",
       " 'order_quantity',\n",
       " 'product_base_margin',\n",
       " 'product_category',\n",
       " 'product_container',\n",
       " 'product_name',\n",
       " 'product_sub_category',\n",
       " 'profit',\n",
       " 'region',\n",
       " 'sales',\n",
       " 'ship_date',\n",
       " 'ship_mode',\n",
       " 'shipping_cost',\n",
       " 'state',\n",
       " 'unit_price',\n",
       " 'zip_code']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df12.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf0b2b",
   "metadata": {},
   "source": [
    "# Checking for null values to confirm if file is ready for further work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49ffab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'city' has 0 null values\n",
      "Column 'customer_age' has 0 null values\n",
      "Column 'customer_name' has 0 null values\n",
      "Column 'customer_segment' has 0 null values\n",
      "Column 'discount' has 0 null values\n",
      "Column 'order_date' has 0 null values\n",
      "Column 'order_id' has 0 null values\n",
      "Column 'order_priority' has 0 null values\n",
      "Column 'order_quantity' has 0 null values\n",
      "Column 'product_base_margin' has 0 null values\n",
      "Column 'product_category' has 0 null values\n",
      "Column 'product_container' has 0 null values\n",
      "Column 'product_name' has 0 null values\n",
      "Column 'product_sub_category' has 0 null values\n",
      "Column 'profit' has 0 null values\n",
      "Column 'region' has 0 null values\n",
      "Column 'sales' has 0 null values\n",
      "Column 'ship_date' has 0 null values\n",
      "Column 'ship_mode' has 0 null values\n",
      "Column 'shipping_cost' has 0 null values\n",
      "Column 'state' has 0 null values\n",
      "Column 'unit_price' has 0 null values\n",
      "Column 'zip_code' has 0 null values\n"
     ]
    }
   ],
   "source": [
    "null_counts1 = []\n",
    "for column in df12.columns:\n",
    "    null_count1 = df12.select(sum(col(column).isNull().cast(\"int\")).alias(column)).collect()[0][0]\n",
    "    null_counts1.append((column, null_count1))\n",
    "\n",
    "# Printing the null counts for each column\n",
    "for column, null_count1 in null_counts1:\n",
    "    print(f\"Column '{column}' has {null_count1} null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe745801",
   "metadata": {},
   "source": [
    "# Saving final preprocessed dataframe to .csv file on hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e589b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/user/talentum/DBDA_Project/Preprocessing/\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df12.coalesce(1).write.option(\"header\", True).csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eae20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
